{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f880a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a21560b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, dataset_path=None):\n",
    "        self.path = dataset_path\n",
    "\n",
    "    def get_data(self):\n",
    "        data = pd.read_csv(self.path).to_numpy()\n",
    "        X, Y_str = data[:, :-1], data[:, -1]  # remove the target column from the input and extract our targets\n",
    "        # n_classes = len(set(Y_str))\n",
    "        n_examples = len(Y_str)\n",
    "        Y = np.zeros(n_examples)\n",
    "        for i in range(len(Y_str)):\n",
    "            category = Y_str[i]\n",
    "            if category == \"banana\":\n",
    "                Y[i] = 0\n",
    "            elif category == \"carrot\":\n",
    "                Y[i] = 1\n",
    "            elif category == \"cucumber\":\n",
    "                Y[i] = 2\n",
    "            elif category == \"mandarin\":\n",
    "                Y[i] = 3\n",
    "            else:\n",
    "                Y[i] = 4\n",
    "        return X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9d437f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, num_iters=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iters = num_iters\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.loss_hist = []\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def predict(y):\n",
    "        return 1 if y > 0.5 else 0\n",
    "    \n",
    "    def logisticLoss(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "            binary cross entropy\n",
    "        \"\"\"\n",
    "        y0 = y_true * np.log(y_pred)\n",
    "        y1 = (1 - y_true) * np.log(1 - y_pred)\n",
    "        return -np.mean(y0 + y1)\n",
    "    \n",
    "    def train(self, X, Y):\n",
    "        n_examples, n_features = X.shape\n",
    "\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias    = 0\n",
    "        \n",
    "        for i in range(self.num_iters):\n",
    "            y_pred = self.sigmoid(np.dot(X, self.weights) + self.bias)\n",
    "\n",
    "            # gradient of binary cross entropy\n",
    "            y_diff = (y_pred - Y)\n",
    "            self.weights -= self.learning_rate * np.dot(X.T, y_diff) / n_examples\n",
    "            self.bias    -= self.learning_rate * np.mean(y_diff)\n",
    "            # print(f\"---------- WEIGHTS (in step {i + 1}) ----------\")\n",
    "            # print(self.weights)\n",
    "            # print(\"------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f19a6758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 75.19295502  95.64546964  92.35715485 ... 235.         136.69693422\n",
      "   20.32672843]\n",
      " [169.61224747  75.30395898 187.84392166 ... 210.         130.65284701\n",
      "   16.25080312]\n",
      " [157.57231903  73.4237332  193.68634415 ... 193.         111.54250186\n",
      "   19.40190755]\n",
      " ...\n",
      " [117.6336441   47.24371812 118.35143661 ...  61.          95.74628637\n",
      "    8.54891902]\n",
      " [112.62142944  42.76008329 115.11558151 ...  87.         106.43180303\n",
      "    7.07523644]\n",
      " [ 66.81015396  63.37149468  77.40125275 ...  35.          85.86439753\n",
      "    6.53533473]]\n",
      "[0. 0. 0. ... 4. 4. 4.]\n",
      "[1343.98476732  921.44477754 1709.63121991  827.24919638 2192.32148814\n",
      "  739.05915705 1773.49979764 1793.5167179  1796.03594092 1800.02480191\n",
      " 1800.98337346 1801.59322746 1786.3047763  1768.20035605 1881.13066855\n",
      " 1906.34394723 1890.27291667 1857.472979   1858.68544484 1894.94101861\n",
      " 1898.33126195 1880.47366547 1984.32727459 1940.47099727 1851.9882915\n",
      " 1764.12499402 1762.97723702 1877.17565147 1956.12076247 1988.53872268\n",
      " 2040.86528518 1868.56057121 1670.85222592 1571.78875939 1574.52855447\n",
      " 1688.28642845 1899.72184255 2063.22855277 2074.34422387 1809.79247012\n",
      " 1434.618604   1280.49590676 1296.03017162 1487.72825393 1878.71476434\n",
      " 2126.07016906 2081.38619023 1868.90672473 1467.6744971  1230.94606728\n",
      " 1248.1829807  1541.61742742 1954.93643955 2157.89778774 2058.17869023\n",
      " 1937.49327442 1701.22466274 1475.31281848 1493.49849983 1757.32112876\n",
      " 1997.53457906 2122.11214652 2014.61040727 1998.9127348  1918.23982155\n",
      " 1833.80724641 1841.90074624 1939.04502988 2020.71622439 2046.64133794\n",
      " 1483.80320416   89.31880148]\n",
      "12.790177595628165\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"./data/tabular/feature_extraction.csv\"\n",
    "dataset = Dataset(dataset_path)\n",
    "X, Y = dataset.get_data()\n",
    "X = X.astype(float)\n",
    "Y = Y.astype(float)\n",
    "print(X)\n",
    "print(Y)\n",
    "logistic_regression = LogisticRegression()\n",
    "logistic_regression.train(X, Y)\n",
    "print(logistic_regression.weights)\n",
    "print(logistic_regression.bias)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
