{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f880a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a21560b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, train_path=None, val_path=None, test_path=None):\n",
    "        self.train_path = train_path\n",
    "        self.val_path = val_path\n",
    "        self.test_path = test_path\n",
    "    \n",
    "    def load_csv(self, path):\n",
    "        data = pd.read_csv(path).to_numpy()\n",
    "        X, Y_str = data[:, :-1], data[:, -1]  # separate data and target\n",
    "        n_examples = len(Y_str)\n",
    "        Y = np.zeros(n_examples)\n",
    "        for i in range(n_examples):\n",
    "            category = Y_str[i]\n",
    "            if category == \"banana\":\n",
    "                Y[i] = 0\n",
    "            elif category == \"carrot\":\n",
    "                Y[i] = 1\n",
    "            elif category == \"cucumber\":\n",
    "                Y[i] = 2\n",
    "            elif category == \"mandarin\":\n",
    "                Y[i] = 3\n",
    "            else:\n",
    "                Y[i] = 4\n",
    "        \n",
    "        return X.astype(float), Y.astype(float)\n",
    "    \n",
    "    def get_data(self):\n",
    "        X_train, Y_train = self.load_csv(self.train_path)\n",
    "        X_val, Y_val     = self.load_csv(self.val_path)\n",
    "        X_test, Y_test   = self.load_csv(self.test_path)\n",
    "        \n",
    "        return (X_train, Y_train), (X_val, Y_val), (X_test, Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d437f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, num_iters=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iters = num_iters\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.loss_hist = []\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        # not to get RuntimeWarning: overflow encountered in exp\n",
    "        z = np.clip(z, -512, 512)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def predict_raw(self, X):\n",
    "        return np.dot(X, self.weights) + self.bias\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return self.sigmoid(self.predict_raw(X))\n",
    "    \n",
    "    def logisticLoss(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "            binary cross entropy\n",
    "        \"\"\"\n",
    "        # to prevent RuntimeWarning: divide by zero encountered in log and RuntimeWarning: invalid value encountered in multiply\n",
    "        eps = 1e-15\n",
    "        y_pred = np.clip(y_pred, eps, 1 - eps)  # keeps values in (0, 1)\n",
    "        y0 = y_true * np.log(y_pred)\n",
    "        y1 = (1 - y_true) * np.log(1 - y_pred)\n",
    "        return -np.mean(y0 + y1)\n",
    "\n",
    "    \n",
    "    def train(self, X, Y, X_val=None, Y_val=None):\n",
    "        n_examples, n_features = X.shape\n",
    "\n",
    "        self.weights        = np.zeros(n_features)\n",
    "        self.bias           = 0\n",
    "        self.loss_hist      = []\n",
    "        self.val_loss_hist  = []\n",
    "        \n",
    "        for i in range(self.num_iters):\n",
    "            # Forward\n",
    "            y_pred = self.sigmoid(np.dot(X, self.weights) + self.bias)\n",
    "\n",
    "            # Save loss\n",
    "            loss = self.logisticLoss(Y, y_pred)\n",
    "            self.loss_hist.append(loss)\n",
    "\n",
    "            # Save validation loss\n",
    "            if X_val is not None:\n",
    "                y_val_pred = self.predict_proba(X_val)\n",
    "                self.val_loss_hist.append(self.logisticLoss(Y_val, y_val_pred))\n",
    "\n",
    "            # gradient of binary cross entropy\n",
    "            y_diff = (y_pred - Y)\n",
    "            self.weights -= self.learning_rate * np.dot(X.T, y_diff) / n_examples\n",
    "            self.bias    -= self.learning_rate * np.mean(y_diff)\n",
    "            # print(f\"---------- WEIGHTS (in step {i + 1}) ----------\")\n",
    "            # print(self.weights)\n",
    "            # print(\"------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5ad20652",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionOVA:\n",
    "    def __init__(self, learning_rate=0.01, num_iters=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iters = num_iters\n",
    "        self.models = []\n",
    "        self.classes = None\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        self.classes = np.unique(Y)\n",
    "        self.models = []\n",
    "\n",
    "        for cls in self.classes:\n",
    "            print(f\"Train class {cls} vs rest\")\n",
    "            Y_binary = (Y == cls).astype(float)\n",
    "            model = LogisticRegression(learning_rate=self.learning_rate, num_iters=self.num_iters)\n",
    "            model.train(X, Y_binary)\n",
    "            self.models.append(model)\n",
    "\n",
    "    def predict(self, X):\n",
    "        all_probs = []\n",
    "\n",
    "        for model in self.models:\n",
    "            probs = model.predict_proba(X)\n",
    "            all_probs.append(probs)\n",
    "\n",
    "        all_probs = np.column_stack(all_probs)\n",
    "        max_idx = np.argmax(all_probs, axis=1)\n",
    "        return self.classes[max_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f4383e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train class 0.0 vs rest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13021/3967459076.py:12: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train class 1.0 vs rest\n",
      "Train class 2.0 vs rest\n",
      "Train class 3.0 vs rest\n",
      "Train class 4.0 vs rest\n",
      "Train Accuracy: 42.57%\n",
      "Val Accuracy:   40.09%\n",
      "Test Accuracy:  54.04%\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset(\n",
    "    train_path=\"./data/tabular/train_processed.csv\",\n",
    "    val_path=\"./data/tabular/validation_processed.csv\",\n",
    "    test_path=\"./data/tabular/test_processed.csv\"\n",
    ")\n",
    "\n",
    "(X_train, Y_train), (X_val, Y_val), (X_test, Y_test) = dataset.get_data()\n",
    "\n",
    "# Train and evaluate\n",
    "model = LogisticRegressionOVA()  # with predefined values\n",
    "model.train(X_train, Y_train)\n",
    "\n",
    "accuracy = lambda y_true, y_pred: np.mean(y_true == y_pred) * 100\n",
    "\n",
    "print(f\"Train Accuracy: {accuracy(Y_train, model.predict(X_train)):.2f}%\")\n",
    "print(f\"Val Accuracy:   {accuracy(Y_val, model.predict(X_val)):.2f}%\")\n",
    "print(f\"Test Accuracy:  {accuracy(Y_test, model.predict(X_test)):.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".mlvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
