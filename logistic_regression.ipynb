{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f880a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a21560b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, dataset_path=None):\n",
    "        self.path = dataset_path\n",
    "\n",
    "    def get_data(self):\n",
    "        data = pd.read_csv(self.path).to_numpy()\n",
    "        X, Y_str = data[:, :-1], data[:, -1]  # remove the target column from the input and extract our targets\n",
    "        # n_classes = len(set(Y_str))\n",
    "        n_examples = len(Y_str)\n",
    "        Y = np.zeros(n_examples)\n",
    "        for i in range(len(Y_str)):\n",
    "            category = Y_str[i]\n",
    "            if category == \"banana\":\n",
    "                Y[i] = 0\n",
    "            elif category == \"carrot\":\n",
    "                Y[i] = 1\n",
    "            elif category == \"cucumber\":\n",
    "                Y[i] = 2\n",
    "            elif category == \"mandarin\":\n",
    "                Y[i] = 3\n",
    "            else:\n",
    "                Y[i] = 4\n",
    "        return X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d437f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, num_iters=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iters = num_iters\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.loss_hist = []\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def predict_raw(self, X):\n",
    "        return np.dot(X, self.weights) + self.bias\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return self.sigmoid(self.predict_raw(X))\n",
    "    \n",
    "    def logisticLoss(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "            binary cross entropy\n",
    "        \"\"\"\n",
    "        y0 = y_true * np.log(y_pred)\n",
    "        y1 = (1 - y_true) * np.log(1 - y_pred)\n",
    "        return -np.mean(y0 + y1)\n",
    "    \n",
    "    def train(self, X, Y, X_val=None, Y_val=None):\n",
    "        n_examples, n_features = X.shape\n",
    "\n",
    "        self.weights        = np.zeros(n_features)\n",
    "        self.bias           = 0\n",
    "        self.loss_hist      = []\n",
    "        self.val_loss_hist  = []\n",
    "        \n",
    "        for i in range(self.num_iters):\n",
    "            # Forward\n",
    "            y_pred = self.sigmoid(np.dot(X, self.weights) + self.bias)\n",
    "\n",
    "            # Save loss\n",
    "            loss = self.logisticLoss(Y, y_pred)\n",
    "            self.loss_hist.append(loss)\n",
    "\n",
    "            # Save validation loss\n",
    "            if X_val is not None:\n",
    "                y_val_pred = self.predict_proba(X_val)\n",
    "                self.val_loss_hist.append(self.logisticLoss(Y_val, y_val_pred))\n",
    "\n",
    "            # gradient of binary cross entropy\n",
    "            y_diff = (y_pred - Y)\n",
    "            self.weights -= self.learning_rate * np.dot(X.T, y_diff) / n_examples\n",
    "            self.bias    -= self.learning_rate * np.mean(y_diff)\n",
    "            # print(f\"---------- WEIGHTS (in step {i + 1}) ----------\")\n",
    "            # print(self.weights)\n",
    "            # print(\"------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad20652",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionOVA:\n",
    "    def __init__(self, learning_rate=0.01, num_iters=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iters = num_iters\n",
    "        self.models = []\n",
    "        self.classes = None\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        self.classes = np.unique(Y)\n",
    "        self.models = []\n",
    "\n",
    "        for cls in self.classes:\n",
    "            print(f\"Train class {cls} vs rest\")\n",
    "            Y_binary = (Y == cls).astype(float)\n",
    "            model = LogisticRegression(learning_rate=self.learning_rate, num_iters=self.num_iters)\n",
    "            model.train(X, Y_binary)\n",
    "            self.models.append(model)\n",
    "\n",
    "    def predict(self, X):\n",
    "        all_probs = []\n",
    "\n",
    "        for model in self.models:\n",
    "            probs = model.predict_proba(X)\n",
    "            all_probs.append(probs)\n",
    "\n",
    "        all_probs = np.column_stack(all_probs)\n",
    "        max_idx = np.argmax(all_probs, axis=1)\n",
    "        return self.classes[max_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19a6758",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"./data/tabular/feature_extraction.csv\"\n",
    "dataset = Dataset(dataset_path)\n",
    "\n",
    "X, Y = dataset.get_data()\n",
    "X = X.astype(float)\n",
    "Y = Y.astype(float)\n",
    "print(X)\n",
    "print(Y)\n",
    "\n",
    "logistic_regression = LogisticRegressionOVA()\n",
    "logistic_regression.train(X, Y)\n",
    "\n",
    "predictions = logistic_regression.predict(X)\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
