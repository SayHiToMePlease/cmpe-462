{
 "cells": [
  {
   "cell_type": "code",
   "id": "0f880a9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T15:52:12.912287Z",
     "start_time": "2025-11-28T15:52:12.908696Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl"
   ],
   "outputs": [],
   "execution_count": 121
  },
  {
   "cell_type": "code",
   "id": "ac77e339a8b46e1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T15:52:12.963109Z",
     "start_time": "2025-11-28T15:52:12.959539Z"
    }
   },
   "source": [
    "SEED = 462\n",
    "np.random.seed(SEED)"
   ],
   "outputs": [],
   "execution_count": 122
  },
  {
   "cell_type": "code",
   "id": "5240eb93449fa9bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T15:52:13.009192Z",
     "start_time": "2025-11-28T15:52:13.007063Z"
    }
   },
   "source": [
    "data_path = os.path.join(\"data\", \"tabular\")"
   ],
   "outputs": [],
   "execution_count": 123
  },
  {
   "cell_type": "code",
   "id": "9a21560b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T15:52:13.054639Z",
     "start_time": "2025-11-28T15:52:13.051239Z"
    }
   },
   "source": [
    "class Dataset:\n",
    "    def __init__(self, train_path, val_path, test_path):\n",
    "        self.train_path = train_path\n",
    "        self.val_path = val_path\n",
    "        self.test_path = test_path\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "        self.label_map = None\n",
    "\n",
    "    def load_csv(self, path):\n",
    "        df = pl.read_csv(path)\n",
    "        data = df.to_numpy()\n",
    "        X = data[:, :-1].astype(float)\n",
    "        Y_str = data[:, -1]\n",
    "        return X, Y_str\n",
    "\n",
    "    def normalize(self, X, fit=False):\n",
    "        if fit:\n",
    "            self.mean = np.mean(X, axis=0)\n",
    "            self.std = np.std(X, axis=0)\n",
    "            self.std[self.std == 0] = 1.0\n",
    "\n",
    "        return (X - self.mean) / self.std\n",
    "\n",
    "    def get_data(self):\n",
    "        X_train, Y_train = self.load_csv(self.train_path)\n",
    "        X_val, Y_val = self.load_csv(self.val_path)\n",
    "        X_test, Y_test = self.load_csv(self.test_path)\n",
    "\n",
    "        X_train = self.normalize(X_train, fit=True)\n",
    "        X_val = self.normalize(X_val, fit=False)\n",
    "        X_test = self.normalize(X_test, fit=False)\n",
    "\n",
    "        return (X_train, Y_train), (X_val, Y_val), (X_test, Y_test)"
   ],
   "outputs": [],
   "execution_count": 124
  },
  {
   "cell_type": "code",
   "id": "9d437f66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T15:52:13.102381Z",
     "start_time": "2025-11-28T15:52:13.098867Z"
    }
   },
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate, num_iters):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iters = num_iters\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.cls = None\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        z = np.clip(z, -500, 500)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.sigmoid(np.dot(X, self.weights) + self.bias)\n",
    "\n",
    "    def logistic_loss(self, y_true, y_pred):\n",
    "        eps = 1e-15\n",
    "        y_pred = np.clip(y_pred, eps, 1 - eps)\n",
    "        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        n_examples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        for i in range(self.num_iters):\n",
    "            y_pred = self.predict_proba(X)\n",
    "            y_diff = y_pred - Y\n",
    "\n",
    "            self.weights -= self.learning_rate * np.dot(X.T, y_diff) / n_examples\n",
    "            self.bias -= self.learning_rate * np.mean(y_diff)"
   ],
   "outputs": [],
   "execution_count": 125
  },
  {
   "cell_type": "code",
   "id": "5ad20652",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T15:52:13.156383Z",
     "start_time": "2025-11-28T15:52:13.148714Z"
    }
   },
   "source": [
    "class LogisticRegressionOVA:\n",
    "    def __init__(self, learning_rate=0.01, num_iters=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iters = num_iters\n",
    "        self.models = []\n",
    "        self.classes = None\n",
    "\n",
    "    def train(self, X_train, Y_train, X_val, Y_val):\n",
    "        self.classes = np.unique(Y_train)\n",
    "        self.models = []\n",
    "\n",
    "        for cls in self.classes:\n",
    "            print(f\"Training for class {cls}:\")\n",
    "            Y_train_bin = (Y_train == cls).astype(float)\n",
    "            Y_val_bin = (Y_val == cls).astype(float)\n",
    "\n",
    "            model = LogisticRegression(self.learning_rate, self.num_iters)\n",
    "            # Custom training loop to print loss\n",
    "            n_examples, n_features = X_train.shape\n",
    "            model.weights = np.zeros(n_features)\n",
    "            model.bias = 0\n",
    "            model.cls = cls\n",
    "\n",
    "            for i in range(model.num_iters):\n",
    "                y_pred = model.predict_proba(X_train)\n",
    "                y_diff = y_pred - Y_train_bin\n",
    "\n",
    "                model.weights -= model.learning_rate * np.dot(X_train.T, y_diff) / n_examples\n",
    "                model.bias -= model.learning_rate * np.mean(y_diff)\n",
    "\n",
    "                if i % 100 == 0:\n",
    "                    train_loss = model.logistic_loss(Y_train_bin, y_pred)\n",
    "                    val_pred = model.predict_proba(X_val)\n",
    "                    val_loss = model.logistic_loss(Y_val_bin, val_pred)\n",
    "                    print(f\"Iter {i}: Train Loss {train_loss:.4f}, Val Loss {val_loss:.4f}\")\n",
    "\n",
    "            self.models.append(model)\n",
    "\n",
    "    def predict(self, X):\n",
    "        probs = np.column_stack([model.predict_proba(X) for model in self.models])\n",
    "        return [self.models[idx].cls for idx in np.argmax(probs, axis=1) ]\n",
    "\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred) * 100"
   ],
   "outputs": [],
   "execution_count": 126
  },
  {
   "cell_type": "code",
   "id": "76d20260",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T15:52:13.206966Z",
     "start_time": "2025-11-28T15:52:13.203374Z"
    }
   },
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred) * 100"
   ],
   "outputs": [],
   "execution_count": 127
  },
  {
   "cell_type": "code",
   "id": "f4383e78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T15:52:14.036899Z",
     "start_time": "2025-11-28T15:52:13.252100Z"
    }
   },
   "source": [
    "if __name__ == \"__main__\":\n",
    "    dataset = Dataset(\n",
    "        train_path=os.path.join(data_path, \"train_processed.csv\"),\n",
    "        val_path=os.path.join(data_path, \"validation_processed.csv\"),\n",
    "        test_path=os.path.join(data_path, \"test_processed.csv\"),\n",
    "    )\n",
    "\n",
    "    (X_train, Y_train), (X_val, Y_val), (X_test, Y_test) = dataset.get_data()\n",
    "\n",
    "    model = LogisticRegressionOVA(learning_rate=0.1, num_iters=1000)\n",
    "    model.train(X_train, Y_train, X_val, Y_val)\n",
    "\n",
    "    test_pred = model.predict(X_test)\n",
    "    print(f\"Test Accuracy: {accuracy(Y_test, test_pred):.2f}%\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for class banana:\n",
      "Iter 0: Train Loss 0.6931, Val Loss 0.6467\n",
      "Iter 100: Train Loss 0.1630, Val Loss 0.1670\n",
      "Iter 200: Train Loss 0.1103, Val Loss 0.1153\n",
      "Iter 300: Train Loss 0.0883, Val Loss 0.0938\n",
      "Iter 400: Train Loss 0.0757, Val Loss 0.0816\n",
      "Iter 500: Train Loss 0.0674, Val Loss 0.0734\n",
      "Iter 600: Train Loss 0.0614, Val Loss 0.0676\n",
      "Iter 700: Train Loss 0.0568, Val Loss 0.0631\n",
      "Iter 800: Train Loss 0.0531, Val Loss 0.0595\n",
      "Iter 900: Train Loss 0.0501, Val Loss 0.0566\n",
      "Training for class carrot:\n",
      "Iter 0: Train Loss 0.6931, Val Loss 0.6626\n",
      "Iter 100: Train Loss 0.1528, Val Loss 0.1567\n",
      "Iter 200: Train Loss 0.0995, Val Loss 0.1032\n",
      "Iter 300: Train Loss 0.0775, Val Loss 0.0808\n",
      "Iter 400: Train Loss 0.0650, Val Loss 0.0681\n",
      "Iter 500: Train Loss 0.0569, Val Loss 0.0597\n",
      "Iter 600: Train Loss 0.0510, Val Loss 0.0537\n",
      "Iter 700: Train Loss 0.0466, Val Loss 0.0491\n",
      "Iter 800: Train Loss 0.0431, Val Loss 0.0456\n",
      "Iter 900: Train Loss 0.0403, Val Loss 0.0426\n",
      "Training for class cucumber:\n",
      "Iter 0: Train Loss 0.6931, Val Loss 0.6370\n",
      "Iter 100: Train Loss 0.0853, Val Loss 0.0916\n",
      "Iter 200: Train Loss 0.0460, Val Loss 0.0511\n",
      "Iter 300: Train Loss 0.0318, Val Loss 0.0362\n",
      "Iter 400: Train Loss 0.0245, Val Loss 0.0284\n",
      "Iter 500: Train Loss 0.0200, Val Loss 0.0236\n",
      "Iter 600: Train Loss 0.0169, Val Loss 0.0203\n",
      "Iter 700: Train Loss 0.0147, Val Loss 0.0180\n",
      "Iter 800: Train Loss 0.0130, Val Loss 0.0162\n",
      "Iter 900: Train Loss 0.0117, Val Loss 0.0148\n",
      "Training for class mandarin:\n",
      "Iter 0: Train Loss 0.6931, Val Loss 0.6647\n",
      "Iter 100: Train Loss 0.2107, Val Loss 0.2060\n",
      "Iter 200: Train Loss 0.1478, Val Loss 0.1444\n",
      "Iter 300: Train Loss 0.1193, Val Loss 0.1164\n",
      "Iter 400: Train Loss 0.1023, Val Loss 0.0998\n",
      "Iter 500: Train Loss 0.0907, Val Loss 0.0885\n",
      "Iter 600: Train Loss 0.0823, Val Loss 0.0803\n",
      "Iter 700: Train Loss 0.0758, Val Loss 0.0739\n",
      "Iter 800: Train Loss 0.0706, Val Loss 0.0688\n",
      "Iter 900: Train Loss 0.0664, Val Loss 0.0647\n",
      "Training for class tomato:\n",
      "Iter 0: Train Loss 0.6931, Val Loss 0.6043\n",
      "Iter 100: Train Loss 0.1107, Val Loss 0.1099\n",
      "Iter 200: Train Loss 0.0717, Val Loss 0.0713\n",
      "Iter 300: Train Loss 0.0563, Val Loss 0.0560\n",
      "Iter 400: Train Loss 0.0478, Val Loss 0.0474\n",
      "Iter 500: Train Loss 0.0424, Val Loss 0.0419\n",
      "Iter 600: Train Loss 0.0385, Val Loss 0.0379\n",
      "Iter 700: Train Loss 0.0356, Val Loss 0.0348\n",
      "Iter 800: Train Loss 0.0333, Val Loss 0.0325\n",
      "Iter 900: Train Loss 0.0314, Val Loss 0.0305\n",
      "Test Accuracy: 95.02%\n"
     ]
    }
   ],
   "execution_count": 128
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".mlvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
