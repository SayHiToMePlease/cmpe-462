{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "89aa4673",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvxopt import matrix, solvers\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "SEED = 462\n",
    "np.random.seed(SEED)\n",
    "\n",
    "data_path = os.path.join(\"..\", \"data_workflow_notebooks\", \"data\", \"tabular\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad97bda",
   "metadata": {},
   "source": [
    "## Soft-Margin SVM\n",
    "\n",
    "$$\n",
    "min_\\alpha \\frac{1}{2}\\sum_{m=1}^N\\sum_{n=1}^N\\alpha_n\\alpha_my_ny_mx_n^Tx_m - \\sum_{n=1}^N\\alpha_n\n",
    "$$\n",
    "\n",
    "$$\n",
    "s.t.\\ \\sum_{n=1}^Ny_n\\alpha_n = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\ \\ \\ \\ \\ \\ 0 \\leq \\alpha_n \\leq C\n",
    "$$\n",
    "\n",
    "$$\n",
    "n=1,\\dots,N\n",
    "$$\n",
    "\n",
    "## QP Solver Representation\n",
    "\n",
    "```python3\n",
    "solvers.qp(Q, p, G, h, A, b)\n",
    "```\n",
    "\n",
    "Where the problem is:\n",
    "$$\n",
    "min_x \\frac{1}{2}x^TQx + p^Tx\n",
    "$$\n",
    "\n",
    "$$\n",
    "s.t.\\ Gx \\leq h\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\ \\ \\ \\ \\ \\ Ax = b\n",
    "$$\n",
    "\n",
    "## What to do\n",
    "\n",
    "We need to convert Soft-Margin SVM representation into QP solver one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d74a5436",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    \"\"\"\n",
    "    This class is taken directly from our previous submission.\n",
    "    \"\"\"\n",
    "    def __init__(self, train_path=None, test_path=None):\n",
    "        self.train_path = train_path\n",
    "        self.test_path = test_path\n",
    "    \n",
    "    def load_csv(self, path):\n",
    "        data = pd.read_csv(path).to_numpy()\n",
    "        X, Y_str = data[:, :-1], data[:, -1]  # separate data and target\n",
    "        n_examples = len(Y_str)\n",
    "        Y = np.zeros(n_examples)\n",
    "        for i in range(n_examples):\n",
    "            category = Y_str[i]\n",
    "            if category == \"banana\":\n",
    "                Y[i] = 0\n",
    "            elif category == \"carrot\":\n",
    "                Y[i] = 1\n",
    "            elif category == \"cucumber\":\n",
    "                Y[i] = 2\n",
    "            elif category == \"mandarin\":\n",
    "                Y[i] = 3\n",
    "            else:\n",
    "                Y[i] = 4\n",
    "        \n",
    "        return X.astype(float), Y.astype(float)\n",
    "    \n",
    "    def get_data(self):\n",
    "        X_train, Y_train = self.load_csv(self.train_path)\n",
    "        X_test, Y_test   = self.load_csv(self.test_path)\n",
    "        \n",
    "        return (X_train, Y_train), (X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d94f4e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftMarginSVM:\n",
    "    def __init__(self, C):\n",
    "        self.weights = None\n",
    "        self.bias    = None\n",
    "        self.C       = C     # soft margin svm gets closer to hard when C gets greater\n",
    "        self.cls     = None  # this is used in the prediction phase to decide which class is the prediction\n",
    "        \n",
    "    def train(self, X, Y):\n",
    "        n_examples, n_features = X.shape\n",
    "\n",
    "        # we need to calculate Q using y_n*y_m and x_n^Tx_m\n",
    "        Y = Y.reshape(-1, 1)    # to prevent np to give a scalar make it Nx1\n",
    "        Y_mul = np.dot(Y, Y.T)  # NxN\n",
    "        X_mul = np.dot(X, X.T)  # NxN\n",
    "\n",
    "        Q = Y_mul * X_mul\n",
    "        p = (np.ones(n_examples) * -1).reshape(-1, 1)\n",
    "\n",
    "        # G is supposed to be (2N)xN\n",
    "        G_first_half  = np.eye(n_examples) * -1  # greater than or equal to 0\n",
    "        G_second_half = np.eye(n_examples)       # less than or equal to C\n",
    "        G = np.vstack([G_first_half, G_second_half])\n",
    "        h_first_half  = np.zeros(n_examples).reshape(-1, 1)\n",
    "        h_second_half = (np.ones(n_examples) * self.C).reshape(-1, 1)\n",
    "        h = np.vstack([h_first_half, h_second_half])\n",
    "\n",
    "        A = Y.reshape(1, -1)\n",
    "        b = 0.0  # cvxopt expects double-precision\n",
    "\n",
    "        sol=solvers.qp(matrix(Q), matrix(p), matrix(G), matrix(h), matrix(A), matrix(b))\n",
    "\n",
    "        # next: use sol[\"x\"] to extract optimal alphas and then calculate w and b\n",
    "        # calculate w*\n",
    "        alpha_star     = np.array(sol[\"x\"]).reshape(-1, 1)  # Nx1\n",
    "        weighted_label = Y * alpha_star  # results in Nx1\n",
    "        w_star         = np.dot(X.T, weighted_label)  # results in dx1 where d is the n_featuresy\n",
    "        self.weights   = w_star\n",
    "\n",
    "        # calculate b*\n",
    "        epsilon = 1e-6  # since optimizers do not work with total precision\n",
    "        sv_mask = ((alpha_star > epsilon) & (alpha_star < (self.C - epsilon))).flatten()  # flatten is used to prevent IndexError\n",
    "        # numpy needs this to apply the mask row-wise\n",
    "        Y_s     = Y[sv_mask]\n",
    "        X_s     = X[sv_mask]\n",
    "        pred    = np.dot(X_s, w_star)\n",
    "        bias    = Y_s - pred\n",
    "        self.bias = np.mean(bias)\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        return (np.dot(X, self.weights) + self.bias)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.sign(self.decision_function(X))\n",
    "    \n",
    "    def hinge_loss(self, y_true, y_pred):\n",
    "        return np.mean(np.maximum(0, 1 - y_true * y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e188c57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftMarginSVM_OVA:\n",
    "    def __init__(self, C=1):\n",
    "        self.C       = C\n",
    "        self.models  = []\n",
    "        self.classes = None\n",
    "        self.history = None\n",
    "\n",
    "    def train(self, X_train, Y_train):\n",
    "        self.classes = np.unique(Y_train)\n",
    "        self.history = {}\n",
    "\n",
    "        for cls in self.classes:\n",
    "            print(f\"--- training for class {cls} ---\")\n",
    "            Y_train_bin = np.where(Y_train == cls, 1, -1).astype(float)  # ready for binary classification\n",
    "\n",
    "            model = SoftMarginSVM(self.C)\n",
    "            model.cls = cls\n",
    "            # Custom training loop to print loss\n",
    "            n_examples, n_features = X_train.shape\n",
    "\n",
    "            model.train(X_train, Y_train_bin)\n",
    "            y_pred = model.decision_function(X_train)\n",
    "            train_loss = model.hinge_loss(Y_train_bin, y_pred)\n",
    "            self.models.append(model)\n",
    "            self.history[cls] = {\"train_loss\": train_loss}\n",
    "\n",
    "    def decision_functions_all(self, X):\n",
    "        return np.column_stack([model.decision_function(X) for model in self.models])\n",
    "    \n",
    "    def predict(self, X):\n",
    "        decisions = self.decision_functions_all(X)\n",
    "        return [self.models[idx].cls for idx in np.argmax(decisions, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7414736d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== start training with C = 0.1 ==============\n",
      "--- training for class 0.0 ---\n",
      "--- training for class 1.0 ---\n",
      "--- training for class 2.0 ---\n",
      "--- training for class 3.0 ---\n",
      "--- training for class 4.0 ---\n",
      "\n",
      "Total Training Time: 1015.99 seconds\n",
      "============== start training with C = 1 ==============\n",
      "--- training for class 0.0 ---\n",
      "--- training for class 1.0 ---\n",
      "--- training for class 2.0 ---\n",
      "--- training for class 3.0 ---\n",
      "--- training for class 4.0 ---\n",
      "\n",
      "Total Training Time: 1074.54 seconds\n",
      "============== start training with C = 10 ==============\n",
      "--- training for class 0.0 ---\n",
      "--- training for class 1.0 ---\n",
      "--- training for class 2.0 ---\n",
      "--- training for class 3.0 ---\n",
      "--- training for class 4.0 ---\n",
      "\n",
      "Total Training Time: 1332.55 seconds\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset(\n",
    "    train_path=os.path.join(data_path, \"train_processed.csv\"),\n",
    "    test_path=os.path.join(data_path, \"test_processed.csv\")\n",
    ")\n",
    "\n",
    "(X_train, Y_train), (X_test, Y_test) = dataset.get_data()\n",
    "\n",
    "svms   = []\n",
    "C_vals = [0.1, 1, 10]\n",
    "\n",
    "for c in C_vals:\n",
    "    svm = SoftMarginSVM_OVA(C=c)\n",
    "\n",
    "    print(f\"============== start training with C = {c} ==============\")\n",
    "    start_time = time.time()\n",
    "    svm.train(X_train, Y_train)\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    print(f\"\\nTotal Training Time: {training_time:.2f} seconds\")\n",
    "    svms.append(svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "61a41f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== prediction with C = 0.1 ==============\n",
      "Final Test Accuracy: 90.03%\n",
      "============== prediction with C = 1 ==============\n",
      "Final Test Accuracy: 90.70%\n",
      "============== prediction with C = 10 ==============\n",
      "Final Test Accuracy: 91.03%\n"
     ]
    }
   ],
   "source": [
    "for s in svms:\n",
    "    c = s.C\n",
    "    print(f\"============== prediction with C = {c} ==============\")\n",
    "    Y_pred = s.predict(X_test)\n",
    "    Y_pred = np.array(Y_pred)\n",
    "\n",
    "    accuracy = np.mean(Y_pred == Y_test)\n",
    "    print(f\"Final Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    # save\n",
    "    results_df = pd.DataFrame({'True_Label': Y_test, 'Predicted_Label': Y_pred})\n",
    "    results_df.to_csv(f\"svm_predictions_output{c}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f011a264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== start training with C = 0.01 ==============\n",
      "--- training for class 0.0 ---\n",
      "--- training for class 1.0 ---\n",
      "--- training for class 2.0 ---\n",
      "--- training for class 3.0 ---\n",
      "--- training for class 4.0 ---\n",
      "\n",
      "Total Training Time: 774.03 seconds\n",
      "============== start training with C = 100 ==============\n",
      "--- training for class 0.0 ---\n",
      "--- training for class 1.0 ---\n",
      "--- training for class 2.0 ---\n",
      "--- training for class 3.0 ---\n",
      "--- training for class 4.0 ---\n",
      "\n",
      "Total Training Time: 1331.49 seconds\n"
     ]
    }
   ],
   "source": [
    "new_svms   = []\n",
    "new_C_vals = [0.01, 100]\n",
    "\n",
    "for c in new_C_vals:\n",
    "    svm = SoftMarginSVM_OVA(C=c)\n",
    "\n",
    "    print(f\"============== start training with C = {c} ==============\")\n",
    "    start_time = time.time()\n",
    "    svm.train(X_train, Y_train)\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    print(f\"\\nTotal Training Time: {training_time:.2f} seconds\")\n",
    "    new_svms.append(svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5a6c028f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== prediction with C = 0.01 ==============\n",
      "Final Test Accuracy: 83.39%\n",
      "============== prediction with C = 100 ==============\n",
      "Final Test Accuracy: 89.04%\n"
     ]
    }
   ],
   "source": [
    "for s in new_svms:\n",
    "    c = s.C\n",
    "    print(f\"============== prediction with C = {c} ==============\")\n",
    "    Y_pred = s.predict(X_test)\n",
    "    Y_pred = np.array(Y_pred)\n",
    "\n",
    "    accuracy = np.mean(Y_pred == Y_test)\n",
    "    print(f\"Final Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    # save\n",
    "    results_df = pd.DataFrame({'True_Label': Y_test, 'Predicted_Label': Y_pred})\n",
    "    results_df.to_csv(f\"svm_predictions_output{c}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
