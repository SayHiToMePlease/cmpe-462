\documentclass[11pt, a4paper]{article}

% --- 1. Page Layout & Formatting ---
\usepackage{geometry}
\geometry{
    top=25mm,
    bottom=25mm,
    left=25mm,
    right=25mm
}
\usepackage{parskip}  % Adds space between paragraphs (modern look)
\usepackage{setspace} % For line spacing
\onehalfspacing       % 1.5 line spacing (easier to read)

% --- 2. Fonts ---
\usepackage{mathpazo} % Uses Palatino font
\usepackage[T1]{fontenc}

% --- 3. Graphics & Tables ---
\usepackage{graphicx}
\usepackage{float}
\usepackage{booktabs}
\usepackage{caption}

% --- 4. Links & Metadata ---
\usepackage[hidelinks]{hyperref}
\usepackage{url} % <--- ADDED: To display website URLs correctly

\usepackage{multirow}

% --- 5. Title Section ---
\title{\textbf{CMPE 462 - Machine Learning: Assignment 1}}
\author{Asaf Kanlipicak, Ali Sonmez, Saur}
\date{\today}

\begin{document}

\maketitle

\section{Introduction}
This report outlines our work on developing a multi-class classification model for fruit and vegetables. For the project, we needed to create a custom dataset consisting of five distinct classes: banana, carrot, cucumber, mandarin, and tomato. Two primary goals of the project was to understand the mechanics of classification algorithms by building one from scratch and facing with the difficulties of data collection.

In the following sections, we describe how we collected and processed our data (combining images, text, and numerical attributes), detail our custom implementation, and compare its performance against the standard Scikit-learn library.

\section{Dataset}
\subsection*{Examples}

Our dataset includes \textbf{5922} images in total. We have \textbf{301} real photographs and \textbf{5621} generated images.

\begin{table}[h]
    \centering
    \caption{Distribution of Generated and Photograph Images per Class}
    \vspace{0.2cm}
    \begin{tabular}{lrrr}
        \toprule
        \textbf{Class} & \textbf{Generated} & \textbf{Photograph} & \textbf{Total} \\
        \midrule
        Banana   & 1000 & 100 & 1100 \\
        Carrot   & 1173 & 50  & 1223 \\
        Cucumber & 1000 & 50  & 1050 \\
        Mandarin & 1000 & 51  & 1051 \\
        Tomato   & 1448 & 50  & 1498 \\
        \midrule
        \textbf{Total} & \textbf{5621} & \textbf{301} & \textbf{5922} \\
        \bottomrule
    \end{tabular}
    \label{tab:dataset_dist}
\end{table}

We have separated 0.8 of the generated images as our \textbf{training} set and 0.2 of it as our \textbf{validation} set.
To see if a model which is trained on generated images can predict real photographs, we have selected all the photographs as our \textbf{test} set.
Below are a few examples.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth]{figures/gen_ban.png} 
    \caption{A generated banana}
    \label{fig:example}
\end{figure}

\begin{table}[h]
    \centering
    \caption{Example Feature Vector (Class: Banana)}
    \vspace{0.2cm}
    \begin{tabular}{llp{6cm}}
        \toprule
        \textbf{Feature Group} & \textbf{Attributes} & \textbf{Values} \\
        \midrule
        \multirow{2}{*}{Grayscale Histogram} & gray\_000 \dots gray\_005 & 2.0, 2.0, 13.0, 76.0, 77.0, 51.0 \dots \\
                                             & \dots gray\_063 & \dots 205.0 \\
        \midrule
        \multirow{3}{*}{Color Statistics}    & blue\_mean, blue\_std & 73.93, 67.23 \\
                                             & green\_mean, green\_std & 133.87, 84.41 \\
                                             & red\_mean, red\_std & 163.57, 100.53 \\
        \midrule
        Physical Attributes                  & weight, size & 130.93 g, 17.86 cm \\
        \midrule
        Text Description                     & description & "soft yellow" \\
        \midrule
        \textbf{Target Label}                & \textbf{class} & \textbf{banana} \\
        \bottomrule
    \end{tabular}
    \label{tab:feature_example}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth]{figures/gen_car.png} 
    \caption{A generated carrot}
    \label{fig:example}
\end{figure}

\begin{table}[h]
    \centering
    \caption{Example Feature Vector (Class: Carrot)}
    \vspace{0.2cm}
    \begin{tabular}{llp{6cm}}
        \toprule
        \textbf{Feature Group} & \textbf{Attributes} & \textbf{Values} \\
        \midrule
        \multirow{2}{*}{Grayscale Histogram} & gray\_000 \dots gray\_005 & 221.0, 217.0, 208.0, 193.0, 206.0, 202.0 \dots \\
                                             & \dots gray\_063 & \dots 172.0 \\
        \midrule
        \multirow{3}{*}{Color Statistics}    & blue\_mean, blue\_std & 172.12, 57.35 \\
                                             & green\_mean, green\_std & 178.13, 48.76 \\
                                             & red\_mean, red\_std & 190.24, 35.87 \\
        \midrule
        Physical Attributes                  & weight, size & 46.06 g, 11.84 cm \\
        \midrule
        Text Description                     & description & "orange temperate" \\
        \midrule
        \textbf{Target Label}                & \textbf{class} & \textbf{carrot} \\
        \bottomrule
    \end{tabular}
    \label{tab:feature_example_carrot}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth]{figures/gen_cuc.png} 
    \caption{A generated cucumber}
    \label{fig:example}
\end{figure}

\begin{table}[h]
    \centering
    \caption{Example Feature Vector (Class: Cucumber)}
    \vspace{0.2cm}
    \begin{tabular}{llp{6cm}}
        \toprule
        \textbf{Feature Group} & \textbf{Attributes} & \textbf{Values} \\
        \midrule
        \multirow{2}{*}{Grayscale Histogram} & gray\_000 \dots gray\_005 & 175.0, 168.0, 163.0, 159.0, 160.0, 154.0 \dots \\
                                             & \dots gray\_063 & \dots 190.0 \\
        \midrule
        \multirow{3}{*}{Color Statistics}    & blue\_mean, blue\_std & 149.94, 82.32 \\
                                             & green\_mean, green\_std & 173.52, 66.73 \\
                                             & red\_mean, red\_std & 175.36, 70.31 \\
        \midrule
        Physical Attributes                  & weight, size & 287.30 g, 25.95 cm \\
        \midrule
        Text Description                     & description & "temperate long" \\
        \midrule
        \textbf{Target Label}                & \textbf{class} & \textbf{cucumber} \\
        \bottomrule
    \end{tabular}
    \label{tab:feature_example_cucumber}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth]{figures/gen_man.png} 
    \caption{A generated mandarin}
    \label{fig:example}
\end{figure}

\begin{table}[h]
    \centering
    \caption{Example Feature Vector (Class: Mandarin)}
    \vspace{0.2cm}
    \begin{tabular}{llp{6cm}}
        \toprule
        \textbf{Feature Group} & \textbf{Attributes} & \textbf{Values} \\
        \midrule
        \multirow{2}{*}{Grayscale Histogram} & gray\_000 \dots gray\_005 & 186.0, 204.0, 225.0, 234.0, 230.0, 200.0 \dots \\
                                             & \dots gray\_063 & \dots 110.0 \\
        \midrule
        \multirow{3}{*}{Color Statistics}    & blue\_mean, blue\_std & 164.11, 85.86 \\
                                             & green\_mean, green\_std & 190.25, 53.10 \\
                                             & red\_mean, red\_std & 212.07, 44.73 \\
        \midrule
        Physical Attributes                  & weight, size & 76.19 g, 7.87 cm \\
        \midrule
        Text Description                     & description & "sour soft" \\
        \midrule
        \textbf{Target Label}                & \textbf{class} & \textbf{mandarin} \\
        \bottomrule
    \end{tabular}
    \label{tab:feature_example_mandarin}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth]{figures/gen_tom.png} 
    \caption{A generated tomato}
    \label{fig:example}
\end{figure}

\begin{table}[h]
    \centering
    \caption{Example Feature Vector (Class: Tomato)}
    \vspace{0.2cm}
    \begin{tabular}{llp{6cm}}
        \toprule
        \textbf{Feature Group} & \textbf{Attributes} & \textbf{Values} \\
        \midrule
        \multirow{2}{*}{Grayscale Histogram} & gray\_000 \dots gray\_005 & 135.0, 144.0, 122.0, 147.0, 218.0, 159.0 \dots \\
                                             & \dots gray\_063 & \dots 50.0 \\
        \midrule
        \multirow{3}{*}{Color Statistics}    & blue\_mean, blue\_std & 100.99, 72.33 \\
                                             & green\_mean, green\_std & 112.19, 72.33 \\
                                             & red\_mean, red\_std & 130.97, 63.93 \\
        \midrule
        Physical Attributes                  & weight, size & 108.11 g, 4.12 cm \\
        \midrule
        Text Description                     & description & "soft sour" \\
        \midrule
        \textbf{Target Label}                & \textbf{class} & \textbf{tomato} \\
        \bottomrule
    \end{tabular}
    \label{tab:feature_example_tomato}
\end{table}

\subsection*{Data Collection}
To obtain a large dataset we employed two techniques:

\begin{itemize}
    \item \textbf{Generation:} The majority of the data was generated using \textbf{Stability AI's Stable Diffusion 3.5 Medium} model. Our prompts are visible in the generation notebook.
    \item \textbf{Manual Photography:} Additionally, we manually took photographs of the actual fruits and vegetables. These are used as the test set.
\end{itemize}

\subsection*{Preprocessing Pipeline}
We wrote a preprocessing script (\texttt{image\_processing.ipynb}) to standardize the raw inputs before feature extraction. Our pipeline is:

\begin{enumerate}
    \item \textbf{Resizing:} All images, regardless of their original resolution, are resized to a uniform dimension of \textbf{$512 \times 512$ pixels}.
    \item \textbf{Randomization:} A fixed seed (\texttt{SEED = 462}) was used to shuffle the dataset for reproducability concerns.
\end{enumerate}

\subsection*{Dataset Splitting Strategy}
We separated the data as follows:

\begin{itemize}
    \item \textbf{Training \& Validation Sets:} Derived from the \textbf{generated} images. The generated data was shuffled and split with an \textbf{80/20 ratio}:
    \begin{itemize}
        \item \textbf{80\%} allocated to the Training set.
        \item \textbf{20\%} allocated to the Validation set.
    \end{itemize}
    \item \textbf{Test Set:} Composed only of the \textbf{real-world photographs}.
\end{itemize}

With the help of this splitting, we can test an interesting case in which a model is trained using generated images, but tested on the real ones.


\subsection*{Feature Extraction}
We have \textbf{73} features in total, 64 of which is the grayscale representation of the image. The other 8 is related to colors, physical attributes, and semantic information.

\subsubsection*{Grayscale Histogram (8 $\times$ 8 = 64 features)}
We convert each image into a 8$\times$8 grayscale image and give the value of each pixel as a feature to our model. This helps distinguishing between objects with similar colors but different surface textures (e.g., the smooth surface of a tomato vs. the rougher texture of a carrot).

\textbf{How did we extract:} We made use of OpenCV to convert the image into grayscale and resize it to 8$\times$8.

\subsubsection*{Color Statistics (RGB Mean and Standard Deviation)}
Color is the most intuitive discriminator for this specific dataset.
\begin{itemize}
    \item \textbf{Mean:} Captures the dominant color of the object. This is crucial to separate the objects with different colors such as banana and mandarin.
    \item \textbf{Standard Deviation:} Captures the color variance. Even though we are not sure, we think that standard deviation might help the model learn some texture specific relation. (On the surface of a cucumber, there might be frequent dark spots.)
\end{itemize}
\textbf{How did we extract:} We made use of OpenCV to read the color value of each pixel in an image. Afterwards, it was just simple math to calculate the features.

\subsubsection*{Physical Attributes (Weight and Size)}
Weight and size are important features since these help the model to learn, hopefully, the geometric attributes of an object. 
\begin{itemize}
    \item These features define the geometric and physical boundaries of the classes (A cucumber is significantly longer/larger than a mandarin).
\end{itemize}
\textbf{How did we extract:} This might be the trickiest one to extract. We used Gaussian distributions with different mean and variance values for each class to generate weights and sizes. For example, the average weight of a banana is 120 g, with a variance of 16 gÂ² in the code. These values were derived by asking to ChatGPT since there are multiple answers on the Internet, but we want one.

\subsubsection*{Semantic Features (Text Description)}
We have selected a set of words to define different classes. However, we paid attention not to make them easily separable. For example, if an object can have the word ``tropical'' in its text feature, then, another one also can.
\begin{itemize}
    \item We aim to make the model learn the relation between a \textit{bag of words} and a class. With combination of words, it is feasible to separate the objects that share some common words.
\end{itemize}
\textbf{How did we extract:} We created a vocabulary in which we aggregated all unique descriptors (e.g., textures, climates) across all classes. We then represented the text description of each sample as a binary feature vector, where each dimension corresponds to the presence (1) or absence (0) of a specific word from this vocabulary. This resulted in a representation whcih encodes the attributes defined in our vocabulary.\\

To integrate the multi-modal data, we concatenated the normalized feature vectors from the visual, physical, and semantic domains into a single unified vector (in a csv file).

\section{Logistic Regression Implementation}
We have implemented the logistic regression using \texttt{numpy} and run tests comparing with \texttt{Scikit-learn}.

\subsection*{Only Image}
In this experiment, we trained the model using only the flattened grayscale histograms and color statistics (Visual Features).

\subsubsection*{Performance Metrics}
The below table shows the overall metrics. While the model achieved an F1-score of \textbf{74.16\%} on the validation set (generated images), performance dropped to \textbf{32.12\%} on the test set (real photographs). This suggests the model learned visual features specific to the generative process and did not generalize to real-world noise.

\begin{table}[H]
    \centering
    \caption{Performance Metrics for Image-Only Classifier}
    \vspace{0.2cm}
    \begin{tabular}{lccc}
        \toprule
        \textbf{Metric} & \textbf{Training} & \textbf{Validation} & \textbf{Test (Photos)} \\
        \midrule
        Accuracy   & 77.20\% & 75.11\% & 32.89\% \\
        Precision  & 0.7652  & 0.7434  & 0.4359  \\
        Recall     & 0.7665  & 0.7483  & 0.3607  \\
        F1 Score   & 0.7624  & 0.7416  & 0.3212  \\
        \bottomrule
    \end{tabular}
    \label{tab:image_metrics}
\end{table}

\subsubsection*{Class-wise AUC Analysis}
The Area Under the Curve (AUC) scores in Table \ref{tab:image_auc} shows class difficulties.
\begin{itemize}
    \item \textbf{Carrot} was the hardest class consistently (Train AUC 0.77, Test AUC 0.56).
    \item \textbf{Mandarin} showed the best generalization, maintaining a relatively high AUC of 0.81 on real photographs compared to other classes.
\end{itemize}

\begin{table}[H]
    \centering
    \caption{Class-wise AUC Scores (One-vs-All)}
    \vspace{0.2cm}
    \begin{tabular}{lccc}
        \toprule
        \textbf{Class} & \textbf{Train AUC} & \textbf{Val AUC} & \textbf{Test AUC} \\
        \midrule
        Banana   & 0.9434 & 0.9343 & 0.7039 \\
        Carrot   & 0.7761 & 0.7424 & 0.5610 \\
        Cucumber & 0.9736 & 0.9657 & 0.6414 \\
        Mandarin & 0.9000 & 0.9135 & 0.8111 \\
        Tomato   & 0.9767 & 0.9719 & 0.7428 \\
        \bottomrule
    \end{tabular}
    \label{tab:image_auc}
\end{table}

\subsubsection*{Training Visualizations}
Figures \ref{fig:loss_image} and  \ref{fig:roc_image} display the training dynamics. The loss curve shows stable convergence without significant overfitting on the synthetic data.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/plots/image/loss.png}
    \caption{Training and Validation Loss during the optimization process.}
    \label{fig:loss_image}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/plots/image/roc.png}
    \caption{ROC Curves evaluating model performance on the Test Set (Photographs).}
    \label{fig:roc_image}
\end{figure}

\subsection*{Only Numeric}
For this experiment, we utilized only the physical attributes (Weight and Size). These features represent a low-dimensional input space compared to images or text.

\subsubsection*{Performance Metrics}
As shown in Table \ref{tab:numeric_metrics}, the model achieves moderate accuracy. The performance is consistent across Training, Validation, and Test sets. Unlike the image modality, the physical attributes generalize well to real-world data, likely because weight and size attributes are generated in the same way. However, overall results show that these two features alone are not enough for a good classification.

\begin{table}[H]
    \centering
    \caption{Performance Metrics for Numeric-Only Classifier}
    \vspace{0.2cm}
    \begin{tabular}{lccc}
        \toprule
        \textbf{Metric} & \textbf{Training} & \textbf{Validation} & \textbf{Test (Photos)} \\
        \midrule
        Accuracy   & 60.01\% & 62.40\% & 60.80\% \\
        Precision  & 0.5245  & 0.5865  & 0.6614  \\
        Recall     & 0.5585  & 0.5831  & 0.5858  \\
        F1 Score   & 0.5054  & 0.5293  & 0.5161  \\
        \bottomrule
    \end{tabular}
    \label{tab:numeric_metrics}
\end{table}

\subsubsection*{Class-wise AUC Analysis}
The AUC scores (Table \ref{tab:numeric_auc}) indicate that certain classes are easily separable by looking at their physical attributes.
\begin{itemize}
    \item \textbf{Carrot and Banana} have high AUC scores ($>0.90$), suggesting their length-to-weight ratios are unique.
    \item \textbf{Cucumber} proved difficult on the test set, possibly due to high variance in real-world cucumber sizes.
\end{itemize}

\begin{table}[H]
    \centering
    \caption{Class-wise AUC Scores (One-vs-All) - Numeric Only}
    \vspace{0.2cm}
    \begin{tabular}{lccc}
        \toprule
        \textbf{Class} & \textbf{Train AUC} & \textbf{Val AUC} & \textbf{Test AUC} \\
        \midrule
        Banana   & 0.9087 & 0.9203 & 0.9060 \\
        Carrot   & 0.9289 & 0.9212 & 0.9249 \\
        Cucumber & 0.8391 & 0.8452 & 0.7768 \\
        Mandarin & 0.8919 & 0.8825 & 0.9492 \\
        Tomato   & 0.8442 & 0.8593 & 0.8556 \\
        \bottomrule
    \end{tabular}
    \label{tab:numeric_auc}
\end{table}

\subsubsection*{Training Visualizations}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/plots/numeric/loss.png}
    \caption{Training and Validation Loss (Numeric-Only).}
    \label{fig:loss_numeric}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/plots/numeric/roc.png}
    \caption{ROC Curves on Test Set (Numeric-Only).}
    \label{fig:roc_numeric}
\end{figure}

\subsection*{Only Text}
This classifier was trained using binary feature vectors derived from the vocabulary of text descriptions.

\subsubsection*{Performance Metrics}
These features (Table \ref{tab:text_metrics}) outperformed both image and numeric ones, achieving approximately 80\% accuracy on the validation set. Also, it maintained approximately 77\% accuracy on the test set, showing nice generalization.

\begin{table}[H]
    \centering
    \caption{Performance Metrics for Text-Only Classifier}
    \vspace{0.2cm}
    \begin{tabular}{lccc}
        \toprule
        \textbf{Metric} & \textbf{Training} & \textbf{Validation} & \textbf{Test (Photos)} \\
        \midrule
        Accuracy   & 79.78\% & 80.00\% & 77.41\% \\
        Precision  & 0.8170  & 0.8146  & 0.8133  \\
        Recall     & 0.7828  & 0.7853  & 0.7537  \\
        F1 Score   & 0.7831  & 0.7875  & 0.7553  \\
        \bottomrule
    \end{tabular}
    \label{tab:text_metrics}
\end{table}

\subsubsection*{Class-wise AUC Analysis}
AUC scores were consistently high ($>0.95$) for all classes across all splits. This confirms that the semantic descriptions provided in the dataset are highly discriminative even though there is randomization and multiple words.

\begin{table}[H]
    \centering
    \caption{Class-wise AUC Scores (One-vs-All) - Text Only}
    \vspace{0.2cm}
    \begin{tabular}{lccc}
        \toprule
        \textbf{Class} & \textbf{Train AUC} & \textbf{Val AUC} & \textbf{Test AUC} \\
        \midrule
        Banana   & 0.9727 & 0.9692 & 0.9614 \\
        Carrot   & 0.9671 & 0.9651 & 0.9547 \\
        Cucumber & 0.9759 & 0.9764 & 0.9749 \\
        Mandarin & 0.9621 & 0.9652 & 0.9504 \\
        Tomato   & 0.9934 & 0.9947 & 0.9859 \\
        \bottomrule
    \end{tabular}
    \label{tab:text_auc}
\end{table}

\subsubsection*{Training Visualizations}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/plots/text/loss.png}
    \caption{Training and Validation Loss (Text-Only).}
    \label{fig:loss_text}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/plots/text/roc.png}
    \caption{ROC Curves on Test Set (Text-Only).}
    \label{fig:roc_text}
\end{figure}

\subsection*{Fused}
Finally, we trained the classifier on the concatenated feature vector, combining Image, Numeric, and Text features.

\subsubsection*{Performance Metrics}
The fused model achieved the best overall performance. As shown in Table \ref{tab:fused_metrics}, it reached near-perfect accuracy on the training and validation data. On the real-world test set, accuracy remained high at \textbf{88.37\%}, outperforming the single-feature baselines (Image: 32\%, Numeric: 60\%, Text: 77\%).

\begin{table}[H]
    \centering
    \caption{Performance Metrics for Fused Data Classifier}
    \vspace{0.2cm}
    \begin{tabular}{lccc}
        \toprule
        \textbf{Metric} & \textbf{Training} & \textbf{Validation} & \textbf{Test (Photos)} \\
        \midrule
        Accuracy   & 98.47\% & 98.04\% & 88.37\% \\
        Precision  & 0.9845  & 0.9795  & 0.8996  \\
        Recall     & 0.9837  & 0.9792  & 0.8705  \\
        F1 Score   & 0.9840  & 0.9793  & 0.8654  \\
        \bottomrule
    \end{tabular}
    \label{tab:fused_metrics}
\end{table}

\subsubsection*{Class-wise AUC Analysis}
The AUC scores are also well, with nearly all classes achieving $>0.98$ on the test set, indicating the model classifies reliably.

\begin{table}[H]
    \centering
    \caption{Class-wise AUC Scores (One-vs-All) - Fused Data}
    \vspace{0.2cm}
    \begin{tabular}{lccc}
        \toprule
        \textbf{Class} & \textbf{Train AUC} & \textbf{Val AUC} & \textbf{Test AUC} \\
        \midrule
        Banana   & 0.9990 & 0.9989 & 0.9919 \\
        Carrot   & 0.9954 & 0.9903 & 0.9854 \\
        Cucumber & 0.9993 & 0.9989 & 0.9799 \\
        Mandarin & 0.9991 & 0.9996 & 0.9984 \\
        Tomato   & 0.9997 & 0.9994 & 0.9968 \\
        \bottomrule
    \end{tabular}
    \label{tab:fused_auc}
\end{table}

\subsubsection*{Training Visualizations}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/plots/fused/loss.png}
    \caption{Training and Validation Loss (Fused Data).}
    \label{fig:loss_fused}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/plots/fused/roc.png}
    \caption{ROC Curves on Test Set (Fused Data).}
    \label{fig:roc_fused}
\end{figure}

\end{document}